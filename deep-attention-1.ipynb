{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Attention 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro_abc_sorter_e6_deepqkv.py\n",
    "# ABC sorter with \"deeper\" Q/K/V: each head’s Q/K/V weights are produced by\n",
    "# a factorized pair (A,B) and (optionally) gate a full-capacity base weight.\n",
    "# Set STRICT_FACTORIZE=True to use W = softmax(A@B) with no base.\n",
    "\n",
    "import math, random, itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Repro & device -----\n",
    "torch.manual_seed(42); random.seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ----- Config -----\n",
    "USE_POS_EMBED   = True\n",
    "STEPS           = 15000\n",
    "BATCH           = 128\n",
    "BASE_LR         = 1e-3\n",
    "WARMUP_STEPS    = 500\n",
    "\n",
    "# New knobs for the deeper-QKV module\n",
    "RANK            = 3         # factorization rank r\n",
    "SOFTMAX_TEMP    = 1.0       # temperature inside softmax gating\n",
    "STRICT_FACTORIZE= False     # False: gate a base weight (recommended). True: pure softmax(A@B).\n",
    "\n",
    "# ----- Vocab -----\n",
    "vocab = ['', 'a', 'b', 'c']\n",
    "SEP, A, B, C = range(len(vocab))\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "V = len(vocab)\n",
    "\n",
    "# ----- Shapes -----\n",
    "T = 7   # 3 input + SEP + 3 output\n",
    "E = 6   # embedding dim\n",
    "H = 2   # heads\n",
    "assert E % H == 0\n",
    "D = E // H  # per-head dim (=3)\n",
    "\n",
    "# ----- Modules (no biases except LayerNorm) -----\n",
    "token_embed = nn.Embedding(V, E).to(device)\n",
    "pos_embed   = nn.Embedding(T, E).to(device) if USE_POS_EMBED else None\n",
    "\n",
    "class MHADeepQKV(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention where each head's Q/K/V weight matrices W∈R^{E×D}\n",
    "    are produced from two deeper matrices A∈R^{E×r}, B∈R^{r×D} via matmul,\n",
    "    optionally passed through a row-wise softmax (gating), and (by default)\n",
    "    elementwise-gated onto a free base weight to preserve expressivity.\n",
    "\n",
    "    Shapes:\n",
    "      x: (B, T, E)\n",
    "      W_q, W_k, W_v: (H, E, D)\n",
    "      q,k,v: (B, H, T, D)\n",
    "    \"\"\"\n",
    "    def __init__(self, E, H, T, rank=3, temp=1.0, strict=False):\n",
    "        super().__init__()\n",
    "        self.H, self.D, self.E = H, E // H, E\n",
    "        self.rank, self.temp, self.strict = rank, temp, strict\n",
    "\n",
    "        # Factorized parameters per head for Q/K/V\n",
    "        def pair():\n",
    "            A = nn.Parameter(torch.empty(H, E, rank))\n",
    "            B = nn.Parameter(torch.empty(H, rank, self.D))\n",
    "            return A, B\n",
    "        self.qA, self.qB = pair()\n",
    "        self.kA, self.kB = pair()\n",
    "        self.vA, self.vB = pair()\n",
    "\n",
    "        # Optional base weights (full capacity E×D per head)\n",
    "        if not self.strict:\n",
    "            self.q_base = nn.Parameter(torch.empty(H, E, self.D))\n",
    "            self.k_base = nn.Parameter(torch.empty(H, E, self.D))\n",
    "            self.v_base = nn.Parameter(torch.empty(H, E, self.D))\n",
    "        else:\n",
    "            self.register_parameter(\"q_base\", None)\n",
    "            self.register_parameter(\"k_base\", None)\n",
    "            self.register_parameter(\"v_base\", None)\n",
    "\n",
    "        # Standard output projection (shared across heads as usual)\n",
    "        self.o = nn.Linear(E, E, bias=False)\n",
    "\n",
    "        # Causal mask\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(T, T)).unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        def init_pair(A, B):\n",
    "            nn.init.kaiming_uniform_(A, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(B, a=math.sqrt(5))\n",
    "        for A, B in [(self.qA, self.qB), (self.kA, self.kB), (self.vA, self.vB)]:\n",
    "            init_pair(A, B)\n",
    "        if self.q_base is not None:\n",
    "            nn.init.xavier_uniform_(self.q_base)\n",
    "            nn.init.xavier_uniform_(self.k_base)\n",
    "            nn.init.xavier_uniform_(self.v_base)\n",
    "        nn.init.xavier_uniform_(self.o.weight)\n",
    "\n",
    "    def _make_weight(self, A, B, base):\n",
    "        # (H,E,r) @ (H,r,D) -> (H,E,D)\n",
    "        W = torch.matmul(A, B) / max(self.temp, 1e-6)\n",
    "        # row-wise softmax over D (keeps per-input-dim rows normalized)\n",
    "        W = torch.softmax(W, dim=-1)\n",
    "        if base is None:\n",
    "            return W                    # strict variant\n",
    "        else:\n",
    "            return base * W             # gated variant (safer; retains capacity)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, Lt, E = x.shape\n",
    "        Wq = self._make_weight(self.qA, self.qB, self.q_base)\n",
    "        Wk = self._make_weight(self.kA, self.kB, self.k_base)\n",
    "        Wv = self._make_weight(self.vA, self.vB, self.v_base)\n",
    "\n",
    "        # Project with per-head weights: (B,T,E) × (H,E,D) -> (B,H,T,D)\n",
    "        q = torch.einsum('bte,hed->bthd', x, Wq).transpose(1, 2)\n",
    "        k = torch.einsum('bte,hed->bthd', x, Wk).transpose(1, 2)\n",
    "        v = torch.einsum('bte,hed->bthd', x, Wv).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.D)\n",
    "        att = att.masked_fill(self.mask[:, :, :Lt, :Lt] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        y = att @ v                           # (B,H,T,D)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, Lt, E)  # (B,T,E)\n",
    "        y = self.o(y)\n",
    "        if return_attn:\n",
    "            return y, att\n",
    "        return y\n",
    "\n",
    "class FF(nn.Module):\n",
    "    def __init__(self, E, mult=1):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(E, mult*E, bias=False)\n",
    "        self.l2 = nn.Linear(mult*E, E, bias=False)\n",
    "    def forward(self, x): return self.l2(F.relu(self.l1(x)))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, E, H, T):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(E)\n",
    "        self.att = MHADeepQKV(E, H, T, rank=RANK, temp=SOFTMAX_TEMP, strict=STRICT_FACTORIZE)\n",
    "        self.ln2 = nn.LayerNorm(E)\n",
    "        self.ff  = FF(E, mult=1)\n",
    "    def forward(self, x, return_attn=False):\n",
    "        if return_attn:\n",
    "            a, att = self.att(self.ln1(x), return_attn=True)\n",
    "        else:\n",
    "            a = self.att(self.ln1(x)); att=None\n",
    "        x = x + a\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        if return_attn: return x, att\n",
    "        return x\n",
    "\n",
    "block      = Block(E, H, T).to(device)\n",
    "final_norm = nn.LayerNorm(E).to(device)\n",
    "\n",
    "# Weight-tied LM head (no extra params)\n",
    "lm_head = nn.Linear(E, V, bias=False).to(device)\n",
    "lm_head.weight = token_embed.weight\n",
    "\n",
    "# ----- Parameter counting -----\n",
    "def count_params():\n",
    "    def n(p): return sum(x.numel() for x in p)\n",
    "    tok = n(token_embed.parameters())                       # V*E\n",
    "    pos = n(pos_embed.parameters()) if pos_embed else 0     # T*E (optional)\n",
    "\n",
    "    # Attention params:\n",
    "    # - factorized Q/K/V: 3 * H * (E*R + R*D)\n",
    "    # - optional base Q/K/V: 3 * H * (E*D) if not strict\n",
    "    # - output projection: E*E\n",
    "    r = RANK; base = 0 if STRICT_FACTORIZE else (3*H*E*D)\n",
    "    att = 3*H*(E*r + r*D) + base + (E*E)\n",
    "\n",
    "    mlp = n(block.ff.parameters())                          # 2*E*E (since mult=1)\n",
    "    lns = n(block.ln1.parameters()) + n(block.ln2.parameters()) + n(final_norm.parameters()) # 3*(2E)\n",
    "    total = tok + pos + att + mlp + lns\n",
    "    return tok, pos, att, mlp, lns, total\n",
    "\n",
    "# ----- Data -----\n",
    "abc_ids = [stoi['a'], stoi['b'], stoi['c']]\n",
    "def make_batch(B=128):\n",
    "    x = torch.empty((B, T), dtype=torch.long)\n",
    "    for i in range(B):\n",
    "        seq = random.choices(abc_ids, k=3)   # duplicates allowed\n",
    "        toks = seq + [SEP] + sorted(seq)\n",
    "        x[i] = torch.tensor(toks)\n",
    "    return x.to(device)\n",
    "\n",
    "# ----- Forward -----\n",
    "def forward(x, return_attn=False):\n",
    "    B, Lt = x.shape\n",
    "    h = token_embed(x)\n",
    "    if pos_embed is not None:\n",
    "        pos = torch.arange(Lt, device=x.device).unsqueeze(0).expand(B, Lt)\n",
    "        h = h + pos_embed(pos)\n",
    "    if return_attn:\n",
    "        h, att = block(h, return_attn=True)\n",
    "    else:\n",
    "        h = block(h); att=None\n",
    "    h = final_norm(h)\n",
    "    logits = lm_head(h)\n",
    "    if return_attn: return logits, att\n",
    "    return logits\n",
    "\n",
    "# ----- Optimizer + simple LR schedule -----\n",
    "opt = torch.optim.AdamW(\n",
    "    list(token_embed.parameters()) +\n",
    "    (list(pos_embed.parameters()) if pos_embed else []) +\n",
    "    list(block.parameters()) +\n",
    "    list(final_norm.parameters()),\n",
    "    lr=BASE_LR\n",
    ")\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < WARMUP_STEPS:\n",
    "        return BASE_LR * (step + 1) / WARMUP_STEPS\n",
    "    t = (step - WARMUP_STEPS) / max(1, STEPS - WARMUP_STEPS)\n",
    "    return BASE_LR * (0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * t)))\n",
    "\n",
    "# ----- Training -----\n",
    "def train():\n",
    "    tok, pos, att, mlp, lns, total = count_params()\n",
    "    print(f\"Architecture: blocks=1, heads={H}, emb={E}, head_dim={D}, mlp_mult=1, context={T}\")\n",
    "    print(f\"  token_embed: {tok}\")\n",
    "    if pos_embed is not None: print(f\"  pos_embed: {pos}\")\n",
    "    print(f\"  attn(deep QKV + proj): {att}  (rank={RANK}, strict={STRICT_FACTORIZE})\")\n",
    "    print(f\"  mlp: {mlp}\")\n",
    "    print(f\"  layer_norms (2 block + final): {lns}\")\n",
    "    print(\"  lm_head: weight-tied (0 extra)\")\n",
    "    print(f\"Trainable parameters: {total}\")\n",
    "\n",
    "    for step in range(STEPS + 1):\n",
    "        for g in opt.param_groups:\n",
    "            g['lr'] = get_lr(step)\n",
    "\n",
    "        x = make_batch(BATCH)\n",
    "        logits = forward(x)\n",
    "        loss = F.cross_entropy(logits[:, :-1, :].reshape(-1, V), x[:, 1:].reshape(-1))\n",
    "        opt.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(opt.param_groups[0]['params']), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step:05d} | LR: {opt.param_groups[0]['lr']:.5f} | Loss: {loss.item():.4f}\")\n",
    "    print(\"✅ Training complete!\")\n",
    "\n",
    "# ----- Generation (greedy) + TRACE -----\n",
    "@torch.no_grad()\n",
    "def generate_sorted(chars):\n",
    "    ids = [stoi[c] for c in chars]\n",
    "    x = torch.tensor([[*ids, SEP]], dtype=torch.long, device=device)\n",
    "    while x.size(1) < 7:\n",
    "        logits = forward(x)[:, -1, :]\n",
    "        logits[:, SEP] = -float('inf')  # forbid SEP after separator\n",
    "        next_id = int(torch.argmax(logits, dim=-1))\n",
    "        x = torch.cat([x, torch.tensor([[next_id]], device=device)], 1)\n",
    "    return [itos[i] for i in x[0].tolist()[-3:]]\n",
    "\n",
    "def _fmt(ids): return \"[\" + \", \".join(itos[i] for i in ids) + \"]\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def trace(chars):\n",
    "    ids = [stoi[c] for c in chars]\n",
    "    x = torch.tensor([[*ids, SEP]], dtype=torch.long, device=device)\n",
    "    print(\"\\n— AR trace —\")\n",
    "    print(f\"start {_fmt(x[0].tolist())}  (heads={H}, head_dim={D})\")\n",
    "    steps=[]\n",
    "    while x.size(1) < 7:\n",
    "        logits, att = forward(x, return_attn=True)\n",
    "        probs = torch.softmax(logits[:, -1, :], -1)[0]\n",
    "        choice = int(torch.argmax(probs))\n",
    "        steps.append(itos[choice])\n",
    "        print(f\"  step{len(steps)}: p(a)={float(probs[stoi['a']]):.3f}, \"\n",
    "              f\"p(b)={float(probs[stoi['b']]):.3f}, p(c)={float(probs[stoi['c']]):.3f} -> '{itos[choice]}'\")\n",
    "        Hh, Tcur = att.shape[1], x.size(1)\n",
    "        w = att[0, :, -1, :Tcur]  # (H, Tcur)\n",
    "        toks = [itos[i] for i in x[0].tolist()]\n",
    "        for h in range(Hh):\n",
    "            weights = \", \".join(f\"{toks[t]}:{w[h,t].item():.2f}\" for t in range(Tcur))\n",
    "            print(f\"    head{h}: [{weights}]\")\n",
    "        x = torch.cat([x, torch.tensor([[choice]], device=device)], 1)\n",
    "    full = x[0].tolist()\n",
    "    print(f\"  full {_fmt(full)} -> outputs {_fmt(full[-3:])}\")\n",
    "\n",
    "# ----- Evaluation -----\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    triples = list(itertools.product(['a','b','c'], repeat=3))\n",
    "    correct = 0\n",
    "    for t in triples:\n",
    "        pred = generate_sorted(list(t))\n",
    "        tgt  = sorted(list(t))\n",
    "        ok = (pred == tgt); correct += int(ok)\n",
    "        print(f\"{list(t)} -> pred {pred} | tgt {tgt} {'✓' if ok else '✗'}\")\n",
    "    print(f\"Model accuracy on 27 triples: {correct}/27\")\n",
    "    for s in [\"caa\",\"bac\",\"ccb\"]:\n",
    "        trace(list(s))\n",
    "\n",
    "# ----- Minimal text box (Jupyter) / CLI fallback -----\n",
    "def launch_textbox():\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display\n",
    "        tb = widgets.Text(\n",
    "            value='caa',\n",
    "            placeholder='Type 3 letters (e.g., cba) then press Enter',\n",
    "            description=''\n",
    "        )\n",
    "        def _on_submit(change):\n",
    "            s = change.value.strip().lower()\n",
    "            if len(s) == 3 and set(s).issubset({'a','b','c'}):\n",
    "                trace(list(s))\n",
    "            else:\n",
    "                print(\"Please enter exactly 3 chars from {a,b,c}.\")\n",
    "        tb.on_submit(_on_submit)\n",
    "        display(tb)\n",
    "        print(\"Type in the box and press Enter.\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            s = input(\"\\nType three letters a-c (e.g., aca), then Enter: \").strip().lower()\n",
    "            if len(s) == 3 and set(s).issubset({'a','b','c'}):\n",
    "                trace(list(s))\n",
    "            else:\n",
    "                print(\"Please enter exactly 3 chars from {a,b,c}.\")\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "# ----- Run -----\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    evaluate()\n",
    "    launch_textbox()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
