{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaffec9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# micro_abc_sorter_levels.py\n",
    "# Transformer forks for tiny ABC sorter with curriculum (train len=3, test 3..8).\n",
    "# Variants: vanilla, lora (W = W0 + A@B), softmax_gate (scaled), tanh_gate (zero-centered),\n",
    "# and strict_softmax (no base). Accurate param counts, same training loop.\n",
    "\n",
    "import math, random, itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------ Repro & device ------------------------\n",
    "torch.manual_seed(42); random.seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ------------------------ Config ------------------------\n",
    "# Task\n",
    "ALPHABET = ['a','b','c']  # keep 3 for now\n",
    "TRAIN_LEN = 3             # train on 3 inputs\n",
    "TEST_LENS = [3,4,5,6,7,8] # evaluate generalization\n",
    "MAX_LEN   = max(TEST_LENS)\n",
    "DUPLICATES_ALLOWED = True\n",
    "\n",
    "# Model/opt\n",
    "USE_POS_EMBED   = True\n",
    "E = 6                  # embedding dim\n",
    "H = 2                  # heads\n",
    "assert E % H == 0\n",
    "D = E // H\n",
    "MLP_MULT = 1\n",
    "BATCH   = 128\n",
    "STEPS   = 15000\n",
    "BASE_LR = 1e-3\n",
    "WARMUP_STEPS = 500\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "# Deeper-QKV knobs (shared across variants unless overridden)\n",
    "RANK         = 3        # factorization rank r\n",
    "SOFTMAX_TEMP = 1.0      # temperature in AB/τ\n",
    "LORA_ALPHA   = 4.0      # LoRA scaling (used as alpha/r)\n",
    "GATE_ALPHA_INIT = D     # init for per-head alpha in softmax_gate\n",
    "GATE_BETA_INIT  = 1.0   # init for per-head beta in tanh_gate\n",
    "\n",
    "# Context / embeddings: set to handle the longest test\n",
    "T_MAX = 2*MAX_LEN + 1   # input L + SEP + output L\n",
    "\n",
    "# Runs to perform (edit this list to try more/less)\n",
    "RUNS = [\n",
    "    {\"name\": \"vanilla\"},\n",
    "    {\"name\": \"lora\", \"rank\": RANK, \"lora_alpha\": LORA_ALPHA},\n",
    "    {\"name\": \"softmax_gate\", \"rank\": RANK, \"temp\": SOFTMAX_TEMP, \"alpha_init\": GATE_ALPHA_INIT},\n",
    "    # You can uncomment to try these too:\n",
    "    # {\"name\": \"tanh_gate\", \"rank\": RANK, \"temp\": SOFTMAX_TEMP, \"beta_init\": GATE_BETA_INIT},\n",
    "    # {\"name\": \"strict_softmax\", \"rank\": RANK, \"temp\": SOFTMAX_TEMP, \"alpha_init\": GATE_ALPHA_INIT},\n",
    "]\n",
    "\n",
    "# ------------------------ Vocab ------------------------\n",
    "vocab = [''] + ALPHABET     # '' is SEP\n",
    "SEP = 0\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "V = len(vocab)\n",
    "\n",
    "# ------------------------ Data ------------------------\n",
    "def make_batch(B, L):\n",
    "    \"\"\"Return (B, T_MAX) long tensor where the last positions are padding that won't be used.\n",
    "       Sequence format: [x1..xL, SEP, y1..yL] where y is sorted(x) ascending.\"\"\"\n",
    "    x = torch.full((B, 2*L+1), SEP, dtype=torch.long)\n",
    "    ids = list(range(1, len(vocab)))  # a..c as 1..V-1\n",
    "    for i in range(B):\n",
    "        if DUPLICATES_ALLOWED:\n",
    "            seq = random.choices(ids, k=L)\n",
    "        else:\n",
    "            seq = random.sample(ids, k=L)\n",
    "        toks = seq + [SEP] + sorted(seq)\n",
    "        x[i] = torch.tensor(toks, dtype=torch.long)\n",
    "    # pad to T_MAX on the right if needed (model accepts variable Lt<=T_MAX)\n",
    "    if x.size(1) < T_MAX:\n",
    "        pad = torch.full((B, T_MAX - x.size(1)), SEP, dtype=torch.long)\n",
    "        x = torch.cat([x, pad], dim=1)\n",
    "    return x.to(device)\n",
    "\n",
    "# ------------------------ Modules ------------------------\n",
    "class MHAFork(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention where each head's Q/K/V matrix W∈R^{E×D} is\n",
    "    produced per variant:\n",
    "      - 'vanilla':         W = W_base\n",
    "      - 'lora':            W = W_base + (alpha/rank) * (A @ B)\n",
    "      - 'softmax_gate':    W = W_base ⊙ (alpha * softmax(A @ B / temp))      # alpha per head\n",
    "      - 'tanh_gate':       W = W_base ⊙ (1 + beta * tanh(A @ B / temp))      # beta per head\n",
    "      - 'strict_softmax':  W = alpha * softmax(A @ B / temp)                  # no base\n",
    "    Shapes:\n",
    "      x: (B,T,E), W: (H,E,D), q/k/v: (B,H,T,D)\n",
    "    \"\"\"\n",
    "    def __init__(self, E, H, T_max, variant=\"vanilla\",\n",
    "                 rank=3, temp=1.0, lora_alpha=4.0,\n",
    "                 alpha_init=D, beta_init=1.0):\n",
    "        super().__init__()\n",
    "        self.H, self.D, self.E = H, E // H, E\n",
    "        self.variant = variant\n",
    "        self.rank = rank\n",
    "        self.temp = temp\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.alpha_init = alpha_init\n",
    "        self.beta_init = beta_init\n",
    "\n",
    "        # Per-head base weights (present in all but strict_softmax)\n",
    "        if variant != \"strict_softmax\":\n",
    "            self.q_base = nn.Parameter(torch.empty(H, E, self.D))\n",
    "            self.k_base = nn.Parameter(torch.empty(H, E, self.D))\n",
    "            self.v_base = nn.Parameter(torch.empty(H, E, self.D))\n",
    "        else:\n",
    "            self.register_parameter(\"q_base\", None)\n",
    "            self.register_parameter(\"k_base\", None)\n",
    "            self.register_parameter(\"v_base\", None)\n",
    "\n",
    "        # Factorized params needed by all but vanilla\n",
    "        if variant in [\"lora\", \"softmax_gate\", \"tanh_gate\", \"strict_softmax\"]:\n",
    "            def pair():\n",
    "                A = nn.Parameter(torch.empty(H, E, rank))\n",
    "                B = nn.Parameter(torch.empty(H, rank, self.D))\n",
    "                return A, B\n",
    "            self.qA, self.qB = pair()\n",
    "            self.kA, self.kB = pair()\n",
    "            self.vA, self.vB = pair()\n",
    "        else:\n",
    "            for name in [\"qA\",\"qB\",\"kA\",\"kB\",\"vA\",\"vB\"]:\n",
    "                self.register_parameter(name, None)\n",
    "\n",
    "        # Head-level scalars for gates\n",
    "        if variant == \"softmax_gate\" or variant == \"strict_softmax\":\n",
    "            # learnable alpha per head, broadcast to (E,D)\n",
    "            self.alpha_q = nn.Parameter(torch.full((H,1,1), float(alpha_init)))\n",
    "            self.alpha_k = nn.Parameter(torch.full((H,1,1), float(alpha_init)))\n",
    "            self.alpha_v = nn.Parameter(torch.full((H,1,1), float(alpha_init)))\n",
    "        else:\n",
    "            for name in [\"alpha_q\",\"alpha_k\",\"alpha_v\"]:\n",
    "                self.register_parameter(name, None)\n",
    "\n",
    "        if variant == \"tanh_gate\":\n",
    "            self.beta_q = nn.Parameter(torch.full((H,1,1), float(beta_init)))\n",
    "            self.beta_k = nn.Parameter(torch.full((H,1,1), float(beta_init)))\n",
    "            self.beta_v = nn.Parameter(torch.full((H,1,1), float(beta_init)))\n",
    "        else:\n",
    "            for name in [\"beta_q\",\"beta_k\",\"beta_v\"]:\n",
    "                self.register_parameter(name, None)\n",
    "\n",
    "        # Output projection\n",
    "        self.o = nn.Linear(E, E, bias=False)\n",
    "\n",
    "        # Causal mask (for max context)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(T_max, T_max))\n",
    "                             .unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        def init_base(W):\n",
    "            if W is not None:\n",
    "                nn.init.xavier_uniform_(W)\n",
    "        init_base(self.q_base); init_base(self.k_base); init_base(self.v_base)\n",
    "        if self.qA is not None:\n",
    "            for A,B in [(self.qA,self.qB),(self.kA,self.kB),(self.vA,self.vB)]:\n",
    "                nn.init.kaiming_uniform_(A, a=math.sqrt(5))\n",
    "                nn.init.kaiming_uniform_(B, a=math.sqrt(5))\n",
    "        nn.init.xavier_uniform_(self.o.weight)\n",
    "\n",
    "    def _AB(self, A, B):\n",
    "        # (H,E,r) @ (H,r,D) -> (H,E,D)\n",
    "        if A is None: return None\n",
    "        return torch.matmul(A, B)\n",
    "\n",
    "    def _make_weight_set(self, A, B, base, alpha=None, beta=None):\n",
    "        var = self.variant\n",
    "        if var == \"vanilla\":\n",
    "            return base\n",
    "        if var == \"lora\":\n",
    "            scale = (self.lora_alpha / max(1, self.rank))\n",
    "            return base + scale * self._AB(A,B)\n",
    "        if var == \"softmax_gate\":\n",
    "            G = torch.softmax(self._AB(A,B) / max(self.temp, 1e-6), dim=-1)\n",
    "            return base * (alpha * G)\n",
    "        if var == \"tanh_gate\":\n",
    "            G = 1.0 + beta * torch.tanh(self._AB(A,B) / max(self.temp, 1e-6))\n",
    "            return base * G\n",
    "        if var == \"strict_softmax\":\n",
    "            G = torch.softmax(self._AB(A,B) / max(self.temp, 1e-6), dim=-1)\n",
    "            return alpha * G\n",
    "        raise ValueError(f\"Unknown variant {var}\")\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, Lt, E = x.shape\n",
    "        # Per-head weights\n",
    "        if self.variant == \"vanilla\":\n",
    "            Wq, Wk, Wv = self.q_base, self.k_base, self.v_base\n",
    "        elif self.variant == \"lora\":\n",
    "            Wq = self._make_weight_set(self.qA, self.qB, self.q_base)\n",
    "            Wk = self._make_weight_set(self.kA, self.kB, self.k_base)\n",
    "            Wv = self._make_weight_set(self.vA, self.vB, self.v_base)\n",
    "        elif self.variant == \"softmax_gate\":\n",
    "            Wq = self._make_weight_set(self.qA, self.qB, self.q_base, self.alpha_q)\n",
    "            Wk = self._make_weight_set(self.kA, self.kB, self.k_base, self.alpha_k)\n",
    "            Wv = self._make_weight_set(self.vA, self.vB, self.v_base, self.alpha_v)\n",
    "        elif self.variant == \"tanh_gate\":\n",
    "            Wq = self._make_weight_set(self.qA, self.qB, self.q_base, beta=self.beta_q)\n",
    "            Wk = self._make_weight_set(self.kA, self.kB, self.k_base, beta=self.beta_k)\n",
    "            Wv = self._make_weight_set(self.vA, self.vB, self.v_base, beta=self.beta_v)\n",
    "        elif self.variant == \"strict_softmax\":\n",
    "            Wq = self._make_weight_set(self.qA, self.qB, None, self.alpha_q)\n",
    "            Wk = self._make_weight_set(self.kA, self.kB, None, self.alpha_k)\n",
    "            Wv = self._make_weight_set(self.vA, self.vB, None, self.alpha_v)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # Projections: (B,T,E) x (H,E,D) -> (B,H,T,D)\n",
    "        q = torch.einsum('bte,hed->bthd', x, Wq).transpose(1, 2)\n",
    "        k = torch.einsum('bte,hed->bthd', x, Wk).transpose(1, 2)\n",
    "        v = torch.einsum('bte,hed->bthd', x, Wv).transpose(1, 2)\n",
    "\n",
    "        # Attention\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.D)\n",
    "        att = att.masked_fill(self.mask[:, :, :Lt, :Lt] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, Lt, E)\n",
    "        y = self.o(y)\n",
    "        if return_attn:\n",
    "            return y, att\n",
    "        return y\n",
    "\n",
    "class FF(nn.Module):\n",
    "    def __init__(self, E, mult=1):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(E, mult*E, bias=False)\n",
    "        self.l2 = nn.Linear(mult*E, E, bias=False)\n",
    "    def forward(self, x): return self.l2(F.relu(self.l1(x)))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, E, H, T_max, variant=\"vanilla\", **kw):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(E)\n",
    "        self.att = MHAFork(E, H, T_max, variant=variant, **kw)\n",
    "        self.ln2 = nn.LayerNorm(E)\n",
    "        self.ff  = FF(E, mult=MLP_MULT)\n",
    "    def forward(self, x, return_attn=False):\n",
    "        if return_attn:\n",
    "            a, att = self.att(self.ln1(x), return_attn=True)\n",
    "        else:\n",
    "            a = self.att(self.ln1(x)); att=None\n",
    "        x = x + a\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        if return_attn: return x, att\n",
    "        return x\n",
    "\n",
    "# ------------------------ Model wrapper ------------------------\n",
    "class SorterModel(nn.Module):\n",
    "    def __init__(self, variant=\"vanilla\", **kw):\n",
    "        super().__init__()\n",
    "        self.variant = variant\n",
    "        self.token_embed = nn.Embedding(V, E).to(device)\n",
    "        if USE_POS_EMBED:\n",
    "            self.pos_embed = nn.Embedding(T_MAX, E).to(device)\n",
    "        else:\n",
    "            self.pos_embed = None\n",
    "        self.block = Block(E, H, T_MAX, variant=variant, **kw).to(device)\n",
    "        self.final_norm = nn.LayerNorm(E).to(device)\n",
    "        # weight-tied LM head\n",
    "        self.lm_head = nn.Linear(E, V, bias=False).to(device)\n",
    "        self.lm_head.weight = self.token_embed.weight\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, Lt = x.shape\n",
    "        h = self.token_embed(x)\n",
    "        if self.pos_embed is not None:\n",
    "            pos = torch.arange(Lt, device=x.device).unsqueeze(0).expand(B, Lt)\n",
    "            h = h + self.pos_embed(pos)\n",
    "        if return_attn:\n",
    "            h, att = self.block(h, return_attn=True)\n",
    "        else:\n",
    "            h = self.block(h); att=None\n",
    "        h = self.final_norm(h)\n",
    "        logits = self.lm_head(h)\n",
    "        if return_attn: return logits, att\n",
    "        return logits\n",
    "\n",
    "# ------------------------ Utilities ------------------------\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < WARMUP_STEPS:\n",
    "        return BASE_LR * (step + 1) / WARMUP_STEPS\n",
    "    t = (step - WARMUP_STEPS) / max(1, STEPS - WARMUP_STEPS)\n",
    "    return BASE_LR * (0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * t)))\n",
    "\n",
    "def train_one(model):\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=BASE_LR)\n",
    "    for step in range(STEPS + 1):\n",
    "        for g in opt.param_groups:\n",
    "            g['lr'] = get_lr(step)\n",
    "        x = make_batch(BATCH, TRAIN_LEN)\n",
    "        logits = model(x)\n",
    "        L = 2*TRAIN_LEN + 1\n",
    "        loss = F.cross_entropy(logits[:, :L-1, :].reshape(-1, V),\n",
    "                               x[:, 1:L].reshape(-1))\n",
    "        opt.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        opt.step()\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step:05d} | LR: {opt.param_groups[0]['lr']:.5f} | Loss: {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sorted(model, chars):\n",
    "    L = len(chars)\n",
    "    ids = [stoi[c] for c in chars]\n",
    "    # pad to T_MAX internally\n",
    "    x = torch.tensor([[*ids, SEP]], dtype=torch.long, device=device)\n",
    "    while x.size(1) < (2*L + 1):\n",
    "        logits = model(x)[:, -1, :]\n",
    "        logits[:, SEP] = -float('inf')  # forbid SEP after the first separator\n",
    "        next_id = int(torch.argmax(logits, dim=-1))\n",
    "        x = torch.cat([x, torch.tensor([[next_id]], device=device)], 1)\n",
    "    return [itos[i] for i in x[0].tolist()[-L:]]\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_lengths(model, lens=TEST_LENS, max_exact=7000, samples=3000):\n",
    "    results = {}\n",
    "    for L in lens:\n",
    "        alphabet = ALPHABET\n",
    "        all_tuples = list(itertools.product(alphabet, repeat=L))\n",
    "        exact = len(all_tuples) <= max_exact\n",
    "        tuples = all_tuples if exact else random.sample(all_tuples, samples)\n",
    "        correct = 0\n",
    "        for t in tuples:\n",
    "            pred = generate_sorted(model, list(t))\n",
    "            tgt  = sorted(list(t))\n",
    "            correct += int(pred == tgt)\n",
    "        acc = correct/len(tuples)\n",
    "        results[L] = (acc, exact, len(tuples))\n",
    "    return results\n",
    "\n",
    "@torch.no_grad()\n",
    "def trace(model, chars):\n",
    "    L = len(chars)\n",
    "    ids = [stoi[c] for c in chars]\n",
    "    x = torch.tensor([[*ids, SEP]], dtype=torch.long, device=device)\n",
    "    print(f\"\\n— AR trace ({model.variant}) —\")\n",
    "    print(f\"start {[itos[i] for i in x[0].tolist()]}  (heads={H}, head_dim={D})\")\n",
    "    steps=[]\n",
    "    while x.size(1) < (2*L + 1):\n",
    "        logits, att = model(x, return_attn=True)\n",
    "        probs = torch.softmax(logits[:, -1, :], -1)[0]\n",
    "        choice = int(torch.argmax(probs))\n",
    "        steps.append(itos[choice])\n",
    "        print(f\"  step{len(steps)}: \" + \", \".join([f\"p({ch})={float(probs[stoi[ch]]):.3f}\" for ch in ALPHABET])\n",
    "              + f\" -> '{itos[choice]}'\")\n",
    "        Hh, Tcur = att.shape[1], x.size(1)\n",
    "        w = att[0, :, -1, :Tcur]  # (H, Tcur)\n",
    "        toks = [itos[i] for i in x[0].tolist()]\n",
    "        for h in range(Hh):\n",
    "            weights = \", \".join(f\"{toks[t]}:{w[h,t].item():.2f}\" for t in range(Tcur))\n",
    "            print(f\"    head{h}: [{weights}]\")\n",
    "        x = torch.cat([x, torch.tensor([[choice]], device=device)], 1)\n",
    "    full = x[0].tolist()\n",
    "    print(f\"  full {[itos[i] for i in full]} -> outputs {[itos[i] for i in full[-L:]]}\")\n",
    "\n",
    "# ------------------------ Experiment harness ------------------------\n",
    "def run_variant(variant_name, **kw):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Variant: {variant_name}\")\n",
    "    m = SorterModel(variant=variant_name, **kw).to(device)\n",
    "    print(f\"Architecture: blocks=1, heads={H}, emb={E}, head_dim={D}, mlp_mult={MLP_MULT}, context(T_MAX)={T_MAX}\")\n",
    "    print(f\"Trainable parameters: {count_params(m)}\")\n",
    "    print(\"Training...\")\n",
    "    train_one(m)\n",
    "    print(\"Evaluating...\")\n",
    "    results = evaluate_lengths(m)\n",
    "    for L in TEST_LENS:\n",
    "        acc, exact, n = results[L]\n",
    "        tag = \"exact\" if exact else f\"sample({n})\"\n",
    "        print(f\"  len={L}: acc={acc:.3f} [{tag}]\")\n",
    "    # Show a couple traces\n",
    "    for s in [\"caa\",\"bac\",\"ccb\"]:\n",
    "        trace(m, list(s))\n",
    "    return variant_name, results\n",
    "\n",
    "def leaderboard(all_results):\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(\"Leaderboard (accuracy)\")\n",
    "    header = \"variant\".ljust(18) + \" | \" + \" \".join([f\"L{L}\".rjust(6) for L in TEST_LENS])\n",
    "    print(header)\n",
    "    print(\"-\"*len(header))\n",
    "    for name, res in all_results:\n",
    "        row = name.ljust(18) + \" | \"\n",
    "        for L in TEST_LENS:\n",
    "            acc = res[L][0]\n",
    "            row += f\"{acc:.3f}\".rjust(6)\n",
    "        print(row)\n",
    "\n",
    "# ------------------------ Main ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    all_results = []\n",
    "    for cfg in RUNS:\n",
    "        name = cfg[\"name\"]\n",
    "        kw = {k:v for k,v in cfg.items() if k != \"name\"}\n",
    "        all_results.append(run_variant(name, **kw))\n",
    "    leaderboard(all_results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
